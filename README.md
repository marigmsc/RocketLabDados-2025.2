# RocketLabDados-2025.2  
RepositÃ³rio Dedicado para DisponibilizaÃ§Ã£o das Atividades do **Programa Rocket Lab de Dados Visagio 2025.2**

## ğŸš€ Atividades de Dados â€“ Fork, Download e ExecuÃ§Ã£o

Bem-vindo(a)! Este repositÃ³rio reÃºne atividades prÃ¡ticas. Para utilizÃ¡-las, faÃ§a um fork deste repositÃ³rio, baixe/clone os materiais a partir do seu fork e importe-os no Databricks para executar as tarefas no ambiente. Este projeto demonstra a construÃ§Ã£o de um **Data Lakehouse** completo, simulando um ambiente produtivo de alta escala. O foco Ã© resolver problemas de negÃ³cio utilizando arquitetura medalhÃ£o, qualidade de dados e orquestraÃ§Ã£o automatizada.

---
## ğŸ§­ Estrutura do RepositÃ³rio

O repositÃ³rio terÃ¡, na estrutura, **uma pasta para cada atividade**, contendo todas as informaÃ§Ãµes necessÃ¡rias para sua realizaÃ§Ã£o (**notebook, datasets, etc.**).

```
/
â”œâ”€â”€ README.md                              # DocumentaÃ§Ã£o Geral do PortfÃ³lio
â”‚
â”œâ”€â”€ Atividade1_Pyspark/                    # MÃ³dulo 1: LÃ³gica e Window Functions
â”‚   â”œâ”€â”€ dados/
â”‚   â”‚   â”œâ”€â”€ fut_players_data.csv
â”‚   â”‚   â”œâ”€â”€ metal_bands.csv
â”‚   â”‚   â””â”€â”€ pokemon_data.csv
â”‚   â”œâ”€â”€ Atividade1_Pyspark.ipynb
â”‚   â””â”€â”€ README.md                          # DocumentaÃ§Ã£o especÃ­fica da Atividade 1
â”‚
â”œâ”€â”€ Atividade2_ETL_Bronze_Silver/          # MÃ³dulo 2: Pipeline ETL (Core)
â”‚   â”œâ”€â”€ Atividade2_Bronze.ipynb            # IngestÃ£o e API
â”‚   â”œâ”€â”€ Atividade2_Silver.ipynb            # Limpeza e Qualidade
â”‚   â””â”€â”€ README.md                          # DocumentaÃ§Ã£o especÃ­fica da Atividade 2
â”‚
â””â”€â”€ Atividade3_Gold_Orquestracao/          # MÃ³dulo 3: Analytics e AutomaÃ§Ã£o
    â”œâ”€â”€ Atividade3_Gold.ipynb              # AgregaÃ§Ãµes para BI
    â”œâ”€â”€ pipeline.yaml                      # Workflow de OrquestraÃ§Ã£o
    â””â”€â”€ README.md                          # DocumentaÃ§Ã£o especÃ­fica da Atividade 3
```
---
## ğŸ›  Tecnologias Utilizadas
* **Databricks** **e** **Databricks Workflow(YAML):** Para orquestraÃ§Ã£o e agendamento de tarefas.
* **Python e PySpark (Spark SQL & DataFrames):** Para processamento distribuÃ­do de dados.
* **Window Functions:** Para criar rankings particionados.
* **Delta Lake:** Armazenamento otimizado com suporte a ACID e Schema Enforcement.
* **Python (Requests):** Consumo de API externa para enriquecimento de dados.

---
## ğŸ§  CompetÃªncias e Habilidades Demonstradas

### 1. Processamento DistribuÃ­do (Big Data)
* **Uso de PySpark:** ImplementaÃ§Ã£o de soluÃ§Ãµes que nÃ£o dependem da memÃ³ria local (como Pandas), preparadas para escalar para Terabytes de dados.
* **OtimizaÃ§Ã£o:** Uso de operaÃ§Ãµes vetoriais e lazy evaluation do Spark.
### 2. Arquitetura de Dados (Medallion Architecture)
* **Desenho de Camadas:** ImplementaÃ§Ã£o prÃ¡tica das camadas **Bronze** (Raw), **Silver** (Clean/Enriched) e Gold (Aggregated).
* **Delta Lake:** Uso do formato Delta para garantir transaÃ§Ãµes ACID e evoluÃ§Ã£o de schema.
* **OrquestraÃ§Ã£o:** Pipeline automatizado definindo dependÃªncias de tarefas para garantir a ordem correta de execuÃ§Ã£o.
* **IntegraÃ§Ã£o de APIs:** Uso da biblioteca Python requests para enriquecer dados internos com fontes externas (Banco Central).
### 3. OtimizaÃ§Ã£o & Big Data
* **Processamento DistribuÃ­do:** CÃ³digos escritos nativamente em Spark para escalar horizontalmente.
* **Window Functions:** SubstituiÃ§Ã£o de loops ineficientes por funÃ§Ãµes de janela analÃ­tica para **rankings e deduplicaÃ§Ãµes.**
*  **Data Cleaning:** Tratamento de tipos de dados e filtragem de ruÃ­dos.
*  **Data Quality:** ImplementaÃ§Ã£o de barreiras de qualidade (ex: validaÃ§Ã£o de UUIDs, datas lÃ³gicas) que impedem dados "sujos" de chegarem Ã  camada Silver.


